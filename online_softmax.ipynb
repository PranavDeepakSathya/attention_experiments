{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca6d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "\n",
    "\n",
    "def safe_softmax(vector): \n",
    "  l = np.size(vector)\n",
    "  out = np.zeros(l)\n",
    "  maxim = float(\"-inf\")\n",
    "  denom = 0 \n",
    "  #first pass, calculate max \n",
    "  for i in range(l): \n",
    "    maxim = max(vector[i],maxim)\n",
    "  #second pass, calculate denom \n",
    "  for i in range(l): \n",
    "    denom += math.exp(vector[i] - maxim)\n",
    "  #third pass, calculate softmax \n",
    "  for i in range(l):\n",
    "    out[i] = math.exp(vector[i]-maxim)/denom\n",
    "    \n",
    "  return out \n",
    "\n",
    "def ref_softmax(vector): \n",
    "  max = np.max(vector)\n",
    "  denom = np.sum(np.exp(vector-max))\n",
    "  return np.exp(vector-max)/denom\n",
    "\n",
    "\n",
    "def safe_online_softmax(vector): \n",
    "  prev_maxim = float(\"-inf\")\n",
    "  prev_denom = 0\n",
    "  maxim = prev_maxim\n",
    "  denom = prev_denom\n",
    "  l = np.size(vector)\n",
    "  out = np.zeros(l)\n",
    "  for i in range(l): \n",
    "    maxim = max(prev_maxim, vector[i])\n",
    "    denom = prev_denom*math.exp(prev_maxim-maxim) + math.exp(vector[i]-maxim)\n",
    "    prev_maxim = maxim \n",
    "    prev_denom = denom \n",
    "    \n",
    "  for i in range(l): \n",
    "    out[i] = math.exp(vector[i]-maxim)/denom\n",
    "  return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29562320",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = np.random.randn(16)\n",
    "ref_out = ref_softmax(vector)\n",
    "out = safe_softmax(vector)\n",
    "online_out = safe_online_softmax(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea9b6777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04818763, 0.28085782, 0.00610432, 0.00608828, 0.0050852 ,\n",
       "       0.02365462, 0.17424583, 0.05015363, 0.11174846, 0.06179297,\n",
       "       0.0120769 , 0.00582275, 0.08914052, 0.0652335 , 0.05237347,\n",
       "       0.00743408])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae8765f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04818763, 0.28085782, 0.00610432, 0.00608828, 0.0050852 ,\n",
       "       0.02365462, 0.17424583, 0.05015363, 0.11174846, 0.06179297,\n",
       "       0.0120769 , 0.00582275, 0.08914052, 0.0652335 , 0.05237347,\n",
       "       0.00743408])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4513ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([float('-inf')]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a8a07c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "971691f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class single_element_attention:\n",
    "  def __init__ (self, D, L_k):\n",
    "    self.Q_row = np.random.randn(1,D)\n",
    "    self.K_T = np.random.randn(D,L_k)\n",
    "    self.S_row = np.zeros((1,L_k))\n",
    "    self.P_row = np.zeros_like(self.S_row)\n",
    "    self.V_col = np.random.randn(L_k,1)\n",
    "\n",
    "  def zero_out(self): \n",
    "    self.S_row = np.zeros_like(self.S_row)\n",
    "    self.P_row = np.zeros_like(self.S_row)\n",
    "    \n",
    "  def naive_attention(self): \n",
    "    out = 0\n",
    "    self.zero_out() \n",
    "    \n",
    "    D,L_k = self.K_T.shape \n",
    "    for lk in range(L_k):\n",
    "      for d in range(D): \n",
    "        self.S_row[0,lk] += self.Q_row[0,d]*self.K_T[d,lk]\n",
    "    maxim = float(\"-inf\")\n",
    "    denom = 0 \n",
    "    for lk in range(L_k): \n",
    "      maxim = max(maxim, self.S_row[0,lk])\n",
    "    for lk in range(L_k): \n",
    "      denom += math.exp((self.S_row[0,lk]-maxim))\n",
    "    for lk in range(L_k): \n",
    "      self.P_row[0,lk] = math.exp((self.S_row[0,lk]-maxim))/denom\n",
    "    for lk in range(L_k): \n",
    "      out += self.P_row[0,lk]*self.V_col[lk,0]\n",
    "      \n",
    "    return out \n",
    "  \n",
    "  def attention_V0(self): \n",
    "    #we dont need to materialize P_row \n",
    "    out = 0\n",
    "    self.zero_out() \n",
    "    \n",
    "    D,L_k = self.K_T.shape \n",
    "    for lk in range(L_k):\n",
    "      for d in range(D): \n",
    "        self.S_row[0,lk] += self.Q_row[0,d]*self.K_T[d,lk]\n",
    "    maxim = float(\"-inf\")\n",
    "    denom = 0 \n",
    "    for lk in range(L_k): \n",
    "      maxim = max(maxim, self.S_row[0,lk])\n",
    "    for lk in range(L_k): \n",
    "      denom += math.exp((self.S_row[0,lk]-maxim))\n",
    "    for lk in range(L_k): \n",
    "     out += (math.exp((self.S_row[0,lk]-maxim))/denom)*self.V_col[lk,0];\n",
    "   \n",
    "    return out \n",
    "  \n",
    "  def attention_V1(self): \n",
    "    #online softmax \n",
    "    out = 0 \n",
    "    self.zero_out()\n",
    "    maxim = float(\"-inf\")\n",
    "    denom = 0 \n",
    "    prev_maxim = maxim \n",
    "    prev_denom = denom \n",
    "    D,L_k = self.K_T.shape \n",
    "    \n",
    "    for lk in range(L_k):\n",
    "      for d in range(D): \n",
    "        self.S_row[0,lk] += self.Q_row[0,d]*self.K_T[d,lk]\n",
    "      maxim = max(prev_maxim, self.S_row[0,lk])\n",
    "      denom = prev_denom*math.exp(prev_maxim-maxim) + math.exp(self.S_row[0,lk]-maxim)\n",
    "      prev_maxim = maxim \n",
    "      prev_denom = denom \n",
    "    \n",
    "    for lk in range(L_k): \n",
    "     out += (math.exp((self.S_row[0,lk]-maxim))/denom)*self.V_col[lk,0]\n",
    "   \n",
    "    return out \n",
    "  \n",
    "  \n",
    "  def attention_V2(self): \n",
    "    #flash\n",
    "    \n",
    "    self.zero_out()\n",
    "    maxim = float(\"-inf\")\n",
    "    denom = 0 \n",
    "    out = 0 \n",
    "    prev_maxim = maxim \n",
    "    prev_denom = denom \n",
    "    prev_out = out\n",
    "    \n",
    "    D,L_k = self.K_T.shape \n",
    "    \n",
    "    for lk in range(L_k):\n",
    "      for d in range(D): \n",
    "        self.S_row[0,lk] += self.Q_row[0,d]*self.K_T[d,lk]\n",
    "      maxim = max(prev_maxim, self.S_row[0,lk])\n",
    "      denom = prev_denom*math.exp(prev_maxim-maxim) + math.exp(self.S_row[0,lk]-maxim)\n",
    "      out = prev_out*math.exp(prev_maxim-maxim) + (math.exp(self.S_row[0,lk]-maxim)*self.V_col[lk,0])\n",
    "\n",
    "      prev_maxim = maxim \n",
    "      prev_denom = denom \n",
    "      prev_out = out\n",
    "\n",
    "    return out/denom \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "212a2265",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 4\n",
    "L_k = 32\n",
    "attn = single_element_attention(D,L_k) \n",
    "\n",
    "naive_out = attn.naive_attention()\n",
    "v0_out = attn.attention_V0()\n",
    "v1_out = attn.attention_V1()\n",
    "v2_out = attn.attention_V2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1861980c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.021774483024603466\n",
      "-0.021774483024603466\n",
      "-0.021774483024603466\n",
      "-0.021774483024603438\n"
     ]
    }
   ],
   "source": [
    "print(naive_out)\n",
    "print(v0_out)\n",
    "print(v1_out)\n",
    "print(v2_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe89a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention: \n",
    "  def __init__ (self, B,Lq,Lkv,D):\n",
    "    self.Q = np.random.randn(B,Lq,D)\n",
    "    self.K = np.random.randn(B,Lkv,D)\n",
    "    self.V = np.random.randn(B,Lkv,D)\n",
    "    \n",
    "  def naive_attention(self): \n",
    "    B,Lq,D = self.Q.shape \n",
    "    _,Lkv,_ = self.K.shape \n",
    "    \n",
    "    S = np.matmul(self.Q, self.K.transpose(0,2,1))\n",
    "    H = np.max(S, axis=2, keepdims=True)\n",
    "    B = np.exp(S-H)\n",
    "    R = np.sum(B, axis=2, keepdims=True)\n",
    "    P = B/R \n",
    "    out = np.matmul(P,self.V)\n",
    "    return out \n",
    "    \n",
    "  def flash_attention(self,tile_Lq, tile_Lkv):\n",
    "    B,Lq,D = self.Q.shape \n",
    "    _,Lkv,_ = self.K.shape \n",
    "    O = np.zeros((B,Lq,D))\n",
    "    \n",
    "    for b in range(B): \n",
    "      for lq_start in range(0,Lq,tile_Lq): \n",
    "        #we are inside a block now \n",
    "        Q_tile = self.Q[b,lq_start:lq_start+tile_Lq,:] #load Q tile once\n",
    "        K_tile = np.zeros((tile_Lkv,D))\n",
    "        S_tile = np.zeros((tile_Lq,tile_Lkv))\n",
    "        P_unorm_tile = np.zeros((tile_Lq,tile_Lkv))\n",
    "        V_tile = np.zeros((tile_Lkv,D))\n",
    "        O_unorm_tile = np.zeros((tile_Lq,D))\n",
    "        \n",
    "        m = np.full((tile_Lq,1),-np.inf)\n",
    "        sum = np.zeros((tile_Lq,1))\n",
    "        \n",
    "        for lkv_start in range(0, Lkv, tile_Lkv): #stream over tile_Lkv chunks \n",
    "          K_tile = self.K[b,lkv_start:lkv_start + tile_Lkv,:] #load k_tile\n",
    "          V_tile = self.V[b,lkv_start:lkv_start + tile_Lkv,:] #load V_tile\n",
    "          S_tile = np.matmul(Q_tile, K_tile.transpose(1,0))\n",
    "          m_p = np.max(S_tile,axis=1,keepdims=True) \n",
    "          m_op_m_p = np.maximum(m,m_p)\n",
    "          P_unorm_tile = np.exp(S_tile-m_p)\n",
    "          sum_p = np.sum(P_unorm_tile,axis=1,keepdims=True)\n",
    "          sum_op_sum_p = (np.exp(m - m_op_m_p)*sum) + (np.exp(m_p - m_op_m_p)*sum_p)\n",
    "          O_unorm_tile_p = np.matmul(P_unorm_tile,V_tile)\n",
    "          O_unorm_tile_op_O_unorm_tile_p = (np.exp(m - m_op_m_p)*O_unorm_tile) + (np.exp(m_p - m_op_m_p)*O_unorm_tile_p)\n",
    "          \n",
    "          #state updates \n",
    "          m = m_op_m_p\n",
    "          sum = sum_op_sum_p\n",
    "          O_unorm_tile = O_unorm_tile_op_O_unorm_tile_p\n",
    "          \n",
    "        \n",
    "        O_norm_tile = O_unorm_tile/sum \n",
    "        O[b,lq_start:lq_start+tile_Lq,:] = O_norm_tile\n",
    "          \n",
    "    return O \n",
    "      \n",
    "  def flash_attention_algorithmic(self,tile_Lq, tile_Lkv):\n",
    "    B,Lq,D = self.Q.shape \n",
    "    _,Lkv,_ = self.K.shape \n",
    "    O = np.zeros((B,Lq,D))\n",
    "    \n",
    "    for b in range(B): \n",
    "      for lq_start in range(0,Lq,tile_Lq): \n",
    "        #we are inside a block now \n",
    "        Q_tile = self.Q[b,lq_start:lq_start+tile_Lq,:] #load Q tile once\n",
    "        K_tile = np.zeros((tile_Lkv,D))\n",
    "        S_tile = np.zeros((tile_Lq,tile_Lkv))\n",
    "        P_unorm_tile = np.zeros((tile_Lq,tile_Lkv))\n",
    "        V_tile = np.zeros((tile_Lkv,D))\n",
    "        O_unorm_tile = np.zeros((tile_Lq,D))\n",
    "        \n",
    "        m = np.full((tile_Lq,1),-np.inf)\n",
    "        sum = np.zeros((tile_Lq,1))\n",
    "        \n",
    "        for lkv_start in range(0, Lkv, tile_Lkv): #stream over tile_Lkv chunks \n",
    "          K_tile = self.K[b,lkv_start:lkv_start + tile_Lkv,:] #load k_tile\n",
    "          V_tile = self.V[b,lkv_start:lkv_start + tile_Lkv,:] #load V_tile\n",
    "          S_tile = np.matmul(Q_tile, K_tile.transpose(1,0))\n",
    "          curr_m = np.max(S_tile,axis=1,keepdims=True) \n",
    "          new_m = np.maximum(m,curr_m)\n",
    "          scale = np.exp(m-new_m)\n",
    "          \n",
    "          P_unorm_tile = np.exp(S_tile-new_m)\n",
    "          curr_sum = np.sum(P_unorm_tile,axis=1,keepdims=True)\n",
    "          sum = (scale*sum) + (curr_sum)\n",
    "          O_unorm_tile *= scale\n",
    "          O_unorm_tile += np.matmul(P_unorm_tile,V_tile)\n",
    "\n",
    "          m = new_m\n",
    "        \n",
    "        O_norm_tile = O_unorm_tile/sum \n",
    "        O[b,lq_start:lq_start+tile_Lq,:] = O_norm_tile\n",
    "          \n",
    "    return O \n",
    "      \n",
    "\n",
    "          \n",
    "          \n",
    "          \n",
    "\n",
    "        \n",
    "        \n",
    "      \n",
    "      \n",
    "      \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6106050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 4\n",
    "Lq = 512\n",
    "Lkv = 1024\n",
    "D = 128 \n",
    "\n",
    "full_attn = attention(B,Lq,Lkv,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7289ee5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3.186340080674199e-14)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_out = full_attn.naive_attention()\n",
    "flash_out = full_attn.flash_attention(64,64)\n",
    "flash_alg_out = full_attn.flash_attention_algorithmic(64,64)\n",
    "np.max(np.abs(naive_out - flash_alg_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2fa1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa99fe70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
